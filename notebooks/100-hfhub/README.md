# Hugging Face Hub

This folder contains notebooks that demonstrate how to use the Hugging Face Hub (Python SDK) to interact programmatically with the Hugging Face Hub to achieve the following tasks:

- Download files from the Hub.
- Upload files to the Hub.
- Manage your repositories.
- Run Inference on deployed models.
- Search for models, datasets and Spaces.
- Share Model Cards to document your models.
- Engage with the community through PRs and comments.

Explore the [Hugging Face Hub Documentation](https://huggingface.co/docs/hub/index) for more details on capabilities and usage.
 - [Homepage](https://huggingface.co/docs/huggingface_hub/index) with top-level collections
 - [How-To Guides](https://huggingface.co/docs/huggingface_hub/guides/overview) grouped by target/task
 - [Inference Guides](https://huggingface.co/docs/huggingface_hub/guides/inference) - using the _InferenceClient_ with:
    - [Inference API](https://huggingface.co/docs/api-inference/index) - serverless usage from HF
    - [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) - managed endpoing you deploy

This image [from the Hugging Face Docs](https://huggingface.co/docs/inference-endpoints/index) shows the default flow for creating and hosting your own Inference server (EP).
![Inference API](./../../assets/img/hf-server-creation-flow.png)

Use this to learn core Hugging Face Hub concepts, explore models and inference tasks for rapid prototyping.

---