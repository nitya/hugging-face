{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>This is a website and blog for documenting my learning journey with Hugging Face models, tools and applications - with three main objectives:</p> <ol> <li>Learn - with a code-first approach and hands-on labs.</li> <li>Share - by authoring blogs &amp; tutorials I can revisit later.</li> <li>Contribute - by providing feedback, fixing bugs or adding docs.</li> </ol> <p>Follow me on LinkedIn and Hugging Face to keep up with updates.</p>"},{"location":"#recent-activity","title":"Recent Activity","text":"<ul> <li>Aug 2024 - New Hugging Face Models on Azure AI: Multilingual, SLM and BioMed</li> </ul>"},{"location":"1-Setup/","title":"Setup Environment","text":"<p>Hugging Face is a community for machine learning and artificial intelligence practitioners, with rich open-source tools and resources for developers. The Hugging Face Hub is the core platform for discovering and exploring repositories, models, datasets, spaces and other features available to developers. Hugging Face also has a number of tools and libraries to support developers:</p> <ol> <li><code>huggingface_hub</code> - Python SDK for Hugging Face Hub interactions.</li> <li><code>llm-vscode</code> - VSCode extension to streamline working with LLM backends.</li> <li>Spaces Dev Mode - (for Pro subscribers) connect VSCode to Docker container in Spaces.</li> <li>Gradio - Python library for building ML &amp; web application demos rapidly.</li> </ol> <p>To jumpstart development, I setup a dev container that has all required tools and dependencies pre-configured. Just launch the container in the cloud (with GitHub Codespaces) or on the local device (with Docker Desktop) - and you're ready to code. Let's validate this.</p>"},{"location":"1-Setup/#1-dev-environment","title":"1. Dev Environment","text":"<p>This repository is instrumented with a <code>devcontainer.json</code> file that pre-installs all Python dependencies listed in the <code>requirements.txt</code> file. </p> <p>1.1 | Launch Dev Container:</p> <p>To get started, *fork the repo to your profile, then pick one of these 2 options to launch your development environment:</p> <ol> <li>Use GitHub Codespaces - launch the dev container in the cloud by selecting <code>Code &gt; Codespaces &gt; Create New Codespace</code> from your fork. This launches a codespaces session in a browser tab.</li> <li>Use Docker Desktop - clone the forked repo to your local device, open in VS Code. Launch Docker Desktop &amp; select <code>Reopen in Container</code> in VS Code (from prompt or command palette).</li> </ol> <p>1.2 | Validate Setup:</p> <p>You should have a Visual Studio Code IDE connected to a runtime that has all required tools and dependencies pre-installed. Open the terminal in VS Code and run the following commands to check:</p> Bash<pre><code># Check Hub Python SDK is installed\npip show huggingface-hub\n\n# Check Hugging Face CLI is installed\nhugginface-cli --help\n</code></pre> <p>1.3 | Set Env Variables:</p> <p>Setup env variables during initial setup by copying the <code>.env.sample</code> file to <code>.env</code>. </p> <p>Bash<pre><code>cp .env.sample .env\n</code></pre> We can then populate it with required values as we go (based on labs requirements). If you run in GitHub Codespaces, you will also get any secrets you associated with the repo auto-injected into the dev container runtime.</p>"},{"location":"1-Setup/#2-learning-resources","title":"2. Learning Resources","text":"<p>Once the development environment is ready, we can start using it to explore various learning resources including:</p> <ol> <li>Open-Source AI Cookbook - recipes for familiar AI tasks and workflows</li> <li>Tokenizers - fast, SOTA tokenization for NLP</li> <li>Datasets - library to access and share datasets</li> <li>Gradio - build applications in a few lines of code </li> <li>Inference API - experiment with 200K+ models on serverless tier</li> <li>Evaluate - assess and report model performance</li> <li>Distilable - synthesize data for AI and add feedback</li> <li>Tasks - understanding tasks taxonomy for inference</li> </ol>"},{"location":"1-Setup/1-hub/","title":"1. Hub Python SDK","text":""},{"location":"1-Setup/1-hub/#learning-resources","title":"Learning Resources","text":"<p>The <code>huggingface_hub</code> library is the primary SDK for working programmatically with the Hugging Face Hub. Here are the three main resources needed to explore it:</p> <ul> <li>Conceptual guides - high-level explainers of the Hub features.</li> <li>API reference - <code>huggingface_hub</code> package classes and methods.</li> <li>How-to guides - practical guides for achieving core tasks</li> </ul>"},{"location":"1-Setup/1-hub/#hands-on-labs","title":"Hands-on Labs","text":"<p>Get Started With Notebooks</p> <p>We'll use Jupyter Notebooks for hands-on labs. It gives us built-in documentation and reproducible code snippets for reuse. To get started with a lab:</p> <ul> <li>Launch the dev container on your fork of the repo</li> <li>Visit the <code>labs/01-huggingface</code> folder in the VS Code file explorer</li> <li>Look for the named lab notebook below and click it to open notebook</li> <li>Click \"Select Kernel\" and pick the default Python environment shown</li> <li>Click \"Clear Outputs\" then \"Run All\" to execute the notebook end-to-end.</li> </ul> <p>Lab Notebooks:</p> <ul> <li>01-hub-quickstart.ipynb \u00b7 Authenticate with Hugging Face \u00b7 Validate basic Hub APIs</li> </ul>"},{"location":"1-Setup/2-vscode/","title":"2. VS Code Extension","text":"<p>The Hugging Face Extension for Visual Studio Code is a great way to interact with the Hub from your development environment. To get started, validate the extension is installed and log into your Hugging Face account.</p> Validate &amp; Authenticate Extension <ol> <li>Type this command to verify the extension is installed:     Bash<pre><code>huggingface-cli --help\n</code></pre></li> <li>Type this command to authenticate with Hugging Face:     Bash<pre><code>huggingface-cli login\n</code></pre></li> <li>Enter your Hugging Face token when prompted</li> <li>Type this command to verify your identity:     Bash<pre><code>huggingface-cli whoami\n</code></pre></li> <li>You should see: your Hugging Face username and organizations</li> </ol>"},{"location":"1-Setup/2-vscode/#21-configure-extension","title":"2.1 Configure Extension","text":"<p>We can now configure the extension to work with different backends including:</p> <ol> <li>Hugging Face Inference API (default)</li> <li>Ollama - get up and running locally with LLMs</li> <li>openai: any OpenAI compatible API (e.g. llama-cpp-python)</li> <li>tgi: Text Generation Inference</li> </ol> TODO \u2192 Explore this further <ul> <li>Read the llm-vscode docs on GitHub.</li> <li>Read the Code Llama blog post for an example use case.</li> </ul>"},{"location":"2-Concepts/","title":"Core Concepts","text":"<p>The generative AI ecosystem evolves rapidly and it can be hard to keep up with all the terms and concepts. This page captures key terms and concepts (as I encounter them) in a FAQ format. Later, I'll expand these into longer posts organized into relevant themes.</p> What is GGUF vs GGML? <p>From this article:</p> <p>GGUF and GGML are file formats used for storing models for inference, especially in the context of language models like GPT (Generative Pre-trained Transformer).</p> <ul> <li>GGML (GPT-Generated Model Language) is a tensor library designed for machine learning, facilitating large models and high performance on various hardware, including Apple Silicon</li> <li>GGUF (GPT-Generated Unified Format) is the successor to GGML. It represents a significant step forward in the field of language model file formats, facilitating enhanced storage and processing of LLMs like GPT.</li> </ul>"},{"location":"3-Models/","title":"Explore: Model Choices","text":""},{"location":"4-Build/","title":"Explore: Inference Tasks","text":""},{"location":"5-Evaluate/","title":"Explore: Benchmarks","text":""},{"location":"blog/","title":"Why This Blog?","text":"<p>I am fascinated by the rapid pace of innovation in the AI ecosystem and particularly in research and open-source communities like Hugging Face. </p> <p>On a personal note, I have ideas I want to explore with AI, and want to use the Hugging Face ecosystem as a way to build my intuition and experiment with new ideas in my account there.</p> <p>On a professional note, I am an AI Advocate at Microsoft, and Hugging Face models are a core part of the Azure AI Model catalog. I want to help developers build intelligent apps, and understand the key tools and metrics available to help with model selection decisions.</p> <p>Let's Go Exploring!</p>"},{"location":"blog/2024/08/08/this-month-in-ai/","title":"This Month in AI","text":"<p>Just a placeholder for now. More to come soon!</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/general/","title":"general","text":""},{"location":"blog/category/azure-ai/","title":"azure-ai","text":""},{"location":"blog/category/this-month/","title":"this-month","text":""}]}